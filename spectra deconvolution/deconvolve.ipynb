{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "e943ee6d-87d8-4e3b-b27e-27a8f698607f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import qr\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.transforms as mtransforms\n",
    "\n",
    "import random\n",
    "import os\n",
    "\n",
    "plt.rc('font', size=14) #controls default text size\n",
    "plt.rc('axes', titlesize=14) #fontsize of the title\n",
    "plt.rc('axes', labelsize=14) #fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=14) #fontsize of the x tick labels\n",
    "plt.rc('ytick', labelsize=14) #fontsize of the y tick labels\n",
    "plt.rc('legend', fontsize=14) #fontsize of the legend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "544e329e-bb57-4d79-8a4a-a2bd4eb063d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def innit_fourpanel_subplots():\n",
    "    \n",
    "    fig, axs = plt.subplot_mosaic([['A)', 'B)'], ['C)', 'D)']],figsize=(14,10.5), constrained_layout=True)\n",
    "\n",
    "    for label, ax in axs.items():\n",
    "        # label physical distance to the left and up:\n",
    "        trans = mtransforms.ScaledTranslation(-20/72, 7/72, fig.dpi_scale_trans)\n",
    "        ax.text(0.0, 1.0, label, transform=ax.transAxes + trans,\n",
    "                fontsize='xx-large', va='bottom')\n",
    "        \n",
    "    axs = [i for i in axs.values()]\n",
    "    fig = fig.set_dpi(400)\n",
    "        \n",
    "    return fig, axs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "id": "39427cfe-00af-40fb-bbc1-521f629e4496",
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_sub(U, y):\n",
    "    \n",
    "    n = U.shape[0]\n",
    "    x = np.zeros(n)\n",
    "    \n",
    "    for i in range(n-1, -1, -1):\n",
    "        s = 0 \n",
    "        for j in range(i+1, n):\n",
    "            s += U[i][j] * x[j]\n",
    "        x[i]= (y[i] - s) / U[i][i]\n",
    "        \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "id": "f5581c16-c0af-4570-a86e-4583c10abee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we start by creating a dictionary which stores our components and allows for easy access\n",
    "def create_component_dict():\n",
    "    ### creates a dictionary which stores each of the components by key ###\n",
    "    component_dict = {}\n",
    "        \n",
    "    for idx, file in enumerate(os.listdir('components')):\n",
    "        \n",
    "        if file == '.ipynb_checkpoints': # ignore jupyter files\n",
    "            pass\n",
    "        else:\n",
    "            # strips the .csv extension and stores the spectra in a array \n",
    "            component_dict[file.strip('.csv')] = pd.read_csv(f'components/{file}', index_col=0).values\n",
    "            \n",
    "    # while we are reading in files it would be useful to get the actual wavelength values as well    \n",
    "    wl =  pd.read_csv(f'components/{file}', index_col=0).index.values\n",
    "    \n",
    "    return component_dict, wl\n",
    "\n",
    "# the component dictionary is created and now we can start reading the experimental spectra\n",
    "component_dict, wl = create_component_dict()\n",
    "\n",
    "# the variable B stores the full set of experimental spectra to be fit.\n",
    "# then each sample can be iterated through. \n",
    "B = pd.read_csv('experimental/experimental_spectra.csv', index_col=0)\n",
    "\n",
    "# as we iterate we want to be able to identify the present components from the sample label\n",
    "def find_present_components(b):\n",
    "    present_components = [] # stores list of dyes in the sample\n",
    "    \n",
    "    comps = b.split('+') # splits the sample name into the dye and concentration\n",
    "    comps = [c.strip(' ') for c in comps] # strips possible spacing\n",
    "    \n",
    "    for c in comps:\n",
    "        dye = c.split(' ')[1] # selects the specific dye name\n",
    "        present_components.append(dye) # adds the dye component to the list \n",
    "        \n",
    "    return present_components\n",
    "\n",
    "# next we need to create our A matrix using the component_dict and components in the sample\n",
    "def create_component_matrix(component_dict, present_components):\n",
    "    \n",
    "    ### first we need to initialize an empty matrix with the proper shape (ie. m x n)\n",
    "    # for clarity \"m\" refers to the the rows which is the sampled wavelength range\n",
    "    #    and \"n\" refers to the number of dye components\n",
    "    \n",
    "    # each component needs to be formatted the same to get the correct m\n",
    "    n = len(present_components)\n",
    "    m = component_dict[present_components[0]].shape[0]\n",
    "    \n",
    "    A = np.empty((m,n))\n",
    "    \n",
    "    ### next we iterate through the present components and add each spectra as column vectors\n",
    "    for i, component in enumerate(present_components):\n",
    "        \n",
    "        # reshape is to make dimensions compatible... idk why its needed though\n",
    "        A[:, i] = (component_dict[component]).reshape(m)\n",
    "        \n",
    "    return A\n",
    "\n",
    "# now we need a way to estimate the weights of each component\n",
    "# to do this we factor the matrix A into two matrices Q and R where Q is orthogonal and R is upper triangular\n",
    "# once we have Q and R we can use back substitution to solve for the optimal component weights\n",
    "# back_sub method is defined above\n",
    "# i think its most numerically stable method, more info below :\n",
    "# https://math.stackexchange.com/questions/3185211/what-does-qr-decomposition-have-to-do-with-least-squares-method\n",
    "def estimate_weights(A, b):\n",
    "    \n",
    "    Q, R = qr(A)\n",
    "    y = np.dot(Q.T, b)\n",
    "    \n",
    "    x = back_sub(R, y)\n",
    "    \n",
    "    return x\n",
    "\n",
    "# we dont really need the weights though, we need the weighted components\n",
    "# so we iterate through the weights and scale the components\n",
    "def scale_components(A, x):\n",
    "    \n",
    "    # initialize new array for weighted components\n",
    "    weighted_A = np.empty(A.shape)\n",
    "    \n",
    "    for idx, x in enumerate(x):\n",
    "        \n",
    "        # weigh the components\n",
    "        weighted_A[:,idx] = A[:,idx] * x\n",
    "    \n",
    "    return weighted_A\n",
    "    \n",
    "# finally we will be interested in calculating the residuals to estimate goodness of fit\n",
    "def measure_residuals(weighted_A, b):\n",
    "    \n",
    "    # sum the weighted components to get fit\n",
    "    fit = weighted_A.sum(axis=1)\n",
    "    \n",
    "    residuals = (b - fit) ** 2\n",
    "    \n",
    "    return residuals\n",
    "\n",
    "# we will want to visualize the fits with plots\n",
    "def plot_fit_data(present_components, A, b_vector, weighted_A, residuals, sample_name):\n",
    "    \n",
    "    fig, ax = innit_fourpanel_subplots()\n",
    "    \n",
    "    # top left - normalized components\n",
    "    ax[0].plot(wl,A/A.max(axis=0))\n",
    "    ax[0].legend(present_components)\n",
    "    ax[0].set_xlabel('Wavelength (nm)')\n",
    "    ax[0].set_ylabel('Emission Norm.')\n",
    "    ax[0].set_title('Normalized Component Spectra')\n",
    "    \n",
    "    # top right - weighted components\n",
    "    ax[1].plot(wl,weighted_A)\n",
    "    ax[1].legend(present_components)\n",
    "    ax[1].set_xlabel('Wavelength (nm)')\n",
    "    ax[1].set_ylabel('Emission (a.u.)')\n",
    "    ax[1].set_title('Weighted Component Spectra')\n",
    "    \n",
    "    # bottom left - experimental vs fit spectra \n",
    "    ax[2].plot(wl, b_vector)\n",
    "    ax[2].plot(wl, weighted_A.sum(axis=1))\n",
    "    ax[2].legend(['Experimental', 'Fit'])\n",
    "    ax[2].set_xlabel('Wavelength (nm)')\n",
    "    ax[2].set_ylabel('Emission (a.u.)')\n",
    "    ax[2].set_title('Experimental vs fit')\n",
    "    \n",
    "    # bottom right - residuals\n",
    "    R2 = 1 - (residuals.sum() / ((b_vector - b_vector.mean())**2).sum())\n",
    "    ax[3].plot(wl,residuals)\n",
    "    ax[3].legend([f'$R^{2}={R2:0.3f}$'])\n",
    "    ax[3].set_xlabel('Wavelength (nm)')\n",
    "    ax[3].set_ylabel('Residual')\n",
    "    ax[3].set_title('Fit residuals')\n",
    "    \n",
    "    plt.savefig(f'plots/{sample_name}.png')\n",
    "    plt.savefig(f'plots/{sample_name}.svg')\n",
    "    plt.show()\n",
    "\n",
    "# in addition to \n",
    "def export_fit(wl, present_components, b_vector, weighted_A, residuals):\n",
    "    \n",
    "    # for adding each of the component spectra and integrated spectra \n",
    "    spectra = {}\n",
    "    spectra_area = {}\n",
    "    \n",
    "    for idx, component in enumerate(present_components):\n",
    "        spectra[f'Fit {component}'] = weighted_A[:,idx]\n",
    "        spectra_area[f'Fit {component} integrated'] = weighted_A[:,idx].sum()\n",
    "    \n",
    "    \n",
    "    data = pd.DataFrame(spectra, index=wl) # initialize df with the weighed component spectra\n",
    "    data.index.name = \"Wavelength (nm)\" # just change the index header to wavelengths\n",
    "    data['Fit full'] = weighted_A.sum(axis=1) # adds the fit\n",
    "    data['Experimental'] = b_vector # adds the original experimental spectra\n",
    "    data['Residuals'] = residuals # adds the residuals at each wavelength\n",
    "    \n",
    "    # because adding dictionaries is annoying af just do this little loop to add the integrated spectra\n",
    "    for key in spectra_area.keys():\n",
    "        data.loc[wl[0],key] = spectra_area[key] \n",
    "        \n",
    "    data.loc[wl[0], 'Fit full Integrated'] = weighted_A.sum(axis=1).sum() # integrate the fit and add that\n",
    "    data.loc[wl[0], 'Sum residuals'] = residuals.sum() # sum of the resiudals i.e. (b - fit)**2.sum()\n",
    "    data.loc[wl[0], 'Coeff of Determination (R2)'] = 1 - (residuals.sum() / ((b_vector - b_vector.mean())**2).sum()) # R squared value for goodness of fit\n",
    "    \n",
    "    return data\n",
    "\n",
    "component_dict, wl = create_component_dict()     \n",
    "\n",
    "# now we can iterate through B and pull out each spectra b and the corresponding component matrix A\n",
    "for b in B.columns:\n",
    "    \n",
    "    present_components = find_present_components(b) # pull out which components are present\n",
    "    A = create_component_matrix(component_dict, present_components) # create the component matrix\n",
    "    b_vector = B[b].values # define experimental spectra \n",
    "    b_vector[b_vector < 0] = 0 ### optional - constrain the experimental spectra to > 0 ###\n",
    "    x = estimate_weights(A, b_vector) # estimate weights \n",
    "    weighted_A = scale_components(A, x) # scale the component vectors\n",
    "    residuals = measure_residuals(weighted_A, b_vector) # caluculate residuals\n",
    "    # plot_fit_data(present_components, A, b_vector, weighted_A, residuals, b) # plot data\n",
    "    data = export_fit(wl, present_components, b_vector, weighted_A, residuals)\n",
    "    data.to_csv(f'exported data/{b} fit data.csv')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019561b6-b0c1-4e2e-a91e-8e59a594f340",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
